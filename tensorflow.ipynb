{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit2843/DLExtras/blob/master/tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlMo6ng1dEvE",
        "colab_type": "code",
        "outputId": "b7ead3f7-deb5-4bb8-95c5-40a4e3166ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!pip install -i https://test.pypi.org/simple/ supportlib"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Collecting supportlib\n",
            "  Downloading https://test-files.pythonhosted.org/packages/c7/e8/a44bb606fca2603f0c79e8593fe0f6f1626dee5bad5177afb9ee260fd223/supportlib-0.1.0-py3-none-any.whl\n",
            "Installing collected packages: supportlib\n",
            "Successfully installed supportlib-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUwTAFetf2N-",
        "colab_type": "code",
        "outputId": "6bf0aea3-55e0-416a-e213-b3b13a48aec9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "import supportlib.gettingdata as getdata\n",
        "getdata.kaggle()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-45c1b570-9fb1-4ff7-9ea1-88947cb91e49\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-45c1b570-9fb1-4ff7-9ea1-88947cb91e49\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvVHkCoogG1V",
        "colab_type": "code",
        "outputId": "a115a8e4-827f-4266-d17f-44518764c798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "!kaggle competitions download -c dog-breed-identification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading labels.csv.zip to /content\n",
            "\r  0% 0.00/214k [00:00<?, ?B/s]\n",
            "100% 214k/214k [00:00<00:00, 88.2MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/281k [00:00<?, ?B/s]\n",
            "100% 281k/281k [00:00<00:00, 86.9MB/s]\n",
            "Downloading test.zip to /content\n",
            " 97% 335M/346M [00:03<00:00, 99.9MB/s]\n",
            "100% 346M/346M [00:03<00:00, 103MB/s] \n",
            "Downloading train.zip to /content\n",
            " 99% 342M/345M [00:03<00:00, 121MB/s]\n",
            "100% 345M/345M [00:03<00:00, 95.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJosT7dRhGlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getdata.zipextract('./train.zip')\n",
        "getdata.zipextract('./labels.csv.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKeJBgN9hXw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('./labels.csv')\n",
        "#data.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI8x-QcfhgB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = {}\n",
        "decoder = {}\n",
        "for i,label in enumerate(data['breed'].unique()):\n",
        "  encoder[label] = i\n",
        "  decoder[i] = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBokZ91wCTb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import cv2\n",
        "import sys\n",
        "class dataset(Dataset):\n",
        "    def __init__(self,data,dir,transform = None):\n",
        "        self.data = data\n",
        "        self.dir = dir\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self,idx):\n",
        "        file = self.data['id'][idx]\n",
        "        path = os.path.join(self.dir,file + '.jpg')\n",
        "        image = cv2.imread(path)\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "        return image,encoder[self.data['breed'][idx]]\n",
        "      \n",
        "im_size = 224\n",
        "mean = [0.4889, 0.4887, 0.4891]\n",
        "std = [0.2074, 0.2074, 0.2074]\n",
        "\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize((im_size,im_size)),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(mean,std)])\n",
        "\n",
        "train_data = dataset(data,'./train',transform = train_transforms)\n",
        "train_loader = DataLoader(train_data,batch_size = 64,num_workers = 8,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBUY0ZYz-nsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(image_batch, label_batch):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(25):\n",
        "      ax = plt.subplot(5,5,n+1)\n",
        "      #image = image_batch[n]*255.0\n",
        "      plt.imshow(image)\n",
        "      plt.title(decoder[label_batch[n].numpy()])\n",
        "      plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urd0gaNUEIAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "batch_size = 16\n",
        "default_timeit_steps = 1000\n",
        "def timeit(ds, steps=default_timeit_steps):\n",
        "  start = time.time()\n",
        "  it = iter(ds)\n",
        "  for i in range(steps):\n",
        "    if(i== len(ds)):\n",
        "      it = iter(ds)\n",
        "    batch = next(it)\n",
        "    if i%10 == 0:\n",
        "      print('.',end='')\n",
        "  print()\n",
        "  end = time.time()\n",
        "  duration = end-start\n",
        "  print(\"{} batches: {} s\".format(steps, duration))\n",
        "  print(\"{:0.5f} Images/s\".format(batch_size*steps/duration))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1jOj9kJEQyf",
        "colab_type": "code",
        "outputId": "3be406bb-7b8c-4404-dc66-d726d117e329",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "timeit(train_loader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "....................................................................................................\n",
            "1000 batches: 83.11957240104675 s\n",
            "192.49377 Images/s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbj2J1GW3fY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-3Ar1R2KCM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class classifie(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classifie, self).__init__()\n",
        "        model = models.densenet121(pretrained = True)\n",
        "        model.classifier = Identity()\n",
        "        self.model = model\n",
        "        self.linear1 = nn.Linear(1024,120)\n",
        "    def forward(self, input):\n",
        "        am = self.model(input)\n",
        "#        x = self.dropout(self.relu(self.linear1(am)))\n",
        "        x = self.linear1(am)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD04WDde5fMv",
        "colab_type": "code",
        "outputId": "17a2cbd7-2542-49dc-9041-b40b41470477",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = classifie().to('cuda')\n",
        "from torchsummary import summary\n",
        "summary(model,(3,224,224))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
            "              ReLU-6           [-1, 64, 56, 56]               0\n",
            "            Conv2d-7          [-1, 128, 56, 56]           8,192\n",
            "       BatchNorm2d-8          [-1, 128, 56, 56]             256\n",
            "              ReLU-9          [-1, 128, 56, 56]               0\n",
            "           Conv2d-10           [-1, 32, 56, 56]          36,864\n",
            "      BatchNorm2d-11           [-1, 96, 56, 56]             192\n",
            "             ReLU-12           [-1, 96, 56, 56]               0\n",
            "           Conv2d-13          [-1, 128, 56, 56]          12,288\n",
            "      BatchNorm2d-14          [-1, 128, 56, 56]             256\n",
            "             ReLU-15          [-1, 128, 56, 56]               0\n",
            "           Conv2d-16           [-1, 32, 56, 56]          36,864\n",
            "      BatchNorm2d-17          [-1, 128, 56, 56]             256\n",
            "             ReLU-18          [-1, 128, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 56, 56]          16,384\n",
            "      BatchNorm2d-20          [-1, 128, 56, 56]             256\n",
            "             ReLU-21          [-1, 128, 56, 56]               0\n",
            "           Conv2d-22           [-1, 32, 56, 56]          36,864\n",
            "      BatchNorm2d-23          [-1, 160, 56, 56]             320\n",
            "             ReLU-24          [-1, 160, 56, 56]               0\n",
            "           Conv2d-25          [-1, 128, 56, 56]          20,480\n",
            "      BatchNorm2d-26          [-1, 128, 56, 56]             256\n",
            "             ReLU-27          [-1, 128, 56, 56]               0\n",
            "           Conv2d-28           [-1, 32, 56, 56]          36,864\n",
            "      BatchNorm2d-29          [-1, 192, 56, 56]             384\n",
            "             ReLU-30          [-1, 192, 56, 56]               0\n",
            "           Conv2d-31          [-1, 128, 56, 56]          24,576\n",
            "      BatchNorm2d-32          [-1, 128, 56, 56]             256\n",
            "             ReLU-33          [-1, 128, 56, 56]               0\n",
            "           Conv2d-34           [-1, 32, 56, 56]          36,864\n",
            "      BatchNorm2d-35          [-1, 224, 56, 56]             448\n",
            "             ReLU-36          [-1, 224, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          28,672\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40           [-1, 32, 56, 56]          36,864\n",
            "      _DenseBlock-41          [-1, 256, 56, 56]               0\n",
            "      BatchNorm2d-42          [-1, 256, 56, 56]             512\n",
            "             ReLU-43          [-1, 256, 56, 56]               0\n",
            "           Conv2d-44          [-1, 128, 56, 56]          32,768\n",
            "        AvgPool2d-45          [-1, 128, 28, 28]               0\n",
            "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
            "             ReLU-47          [-1, 128, 28, 28]               0\n",
            "           Conv2d-48          [-1, 128, 28, 28]          16,384\n",
            "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
            "             ReLU-50          [-1, 128, 28, 28]               0\n",
            "           Conv2d-51           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-52          [-1, 160, 28, 28]             320\n",
            "             ReLU-53          [-1, 160, 28, 28]               0\n",
            "           Conv2d-54          [-1, 128, 28, 28]          20,480\n",
            "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
            "             ReLU-56          [-1, 128, 28, 28]               0\n",
            "           Conv2d-57           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-58          [-1, 192, 28, 28]             384\n",
            "             ReLU-59          [-1, 192, 28, 28]               0\n",
            "           Conv2d-60          [-1, 128, 28, 28]          24,576\n",
            "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
            "             ReLU-62          [-1, 128, 28, 28]               0\n",
            "           Conv2d-63           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-64          [-1, 224, 28, 28]             448\n",
            "             ReLU-65          [-1, 224, 28, 28]               0\n",
            "           Conv2d-66          [-1, 128, 28, 28]          28,672\n",
            "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
            "             ReLU-68          [-1, 128, 28, 28]               0\n",
            "           Conv2d-69           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-70          [-1, 256, 28, 28]             512\n",
            "             ReLU-71          [-1, 256, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]          32,768\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-76          [-1, 288, 28, 28]             576\n",
            "             ReLU-77          [-1, 288, 28, 28]               0\n",
            "           Conv2d-78          [-1, 128, 28, 28]          36,864\n",
            "      BatchNorm2d-79          [-1, 128, 28, 28]             256\n",
            "             ReLU-80          [-1, 128, 28, 28]               0\n",
            "           Conv2d-81           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-82          [-1, 320, 28, 28]             640\n",
            "             ReLU-83          [-1, 320, 28, 28]               0\n",
            "           Conv2d-84          [-1, 128, 28, 28]          40,960\n",
            "      BatchNorm2d-85          [-1, 128, 28, 28]             256\n",
            "             ReLU-86          [-1, 128, 28, 28]               0\n",
            "           Conv2d-87           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-88          [-1, 352, 28, 28]             704\n",
            "             ReLU-89          [-1, 352, 28, 28]               0\n",
            "           Conv2d-90          [-1, 128, 28, 28]          45,056\n",
            "      BatchNorm2d-91          [-1, 128, 28, 28]             256\n",
            "             ReLU-92          [-1, 128, 28, 28]               0\n",
            "           Conv2d-93           [-1, 32, 28, 28]          36,864\n",
            "      BatchNorm2d-94          [-1, 384, 28, 28]             768\n",
            "             ReLU-95          [-1, 384, 28, 28]               0\n",
            "           Conv2d-96          [-1, 128, 28, 28]          49,152\n",
            "      BatchNorm2d-97          [-1, 128, 28, 28]             256\n",
            "             ReLU-98          [-1, 128, 28, 28]               0\n",
            "           Conv2d-99           [-1, 32, 28, 28]          36,864\n",
            "     BatchNorm2d-100          [-1, 416, 28, 28]             832\n",
            "            ReLU-101          [-1, 416, 28, 28]               0\n",
            "          Conv2d-102          [-1, 128, 28, 28]          53,248\n",
            "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
            "            ReLU-104          [-1, 128, 28, 28]               0\n",
            "          Conv2d-105           [-1, 32, 28, 28]          36,864\n",
            "     BatchNorm2d-106          [-1, 448, 28, 28]             896\n",
            "            ReLU-107          [-1, 448, 28, 28]               0\n",
            "          Conv2d-108          [-1, 128, 28, 28]          57,344\n",
            "     BatchNorm2d-109          [-1, 128, 28, 28]             256\n",
            "            ReLU-110          [-1, 128, 28, 28]               0\n",
            "          Conv2d-111           [-1, 32, 28, 28]          36,864\n",
            "     BatchNorm2d-112          [-1, 480, 28, 28]             960\n",
            "            ReLU-113          [-1, 480, 28, 28]               0\n",
            "          Conv2d-114          [-1, 128, 28, 28]          61,440\n",
            "     BatchNorm2d-115          [-1, 128, 28, 28]             256\n",
            "            ReLU-116          [-1, 128, 28, 28]               0\n",
            "          Conv2d-117           [-1, 32, 28, 28]          36,864\n",
            "     _DenseBlock-118          [-1, 512, 28, 28]               0\n",
            "     BatchNorm2d-119          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-120          [-1, 512, 28, 28]               0\n",
            "          Conv2d-121          [-1, 256, 28, 28]         131,072\n",
            "       AvgPool2d-122          [-1, 256, 14, 14]               0\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "            ReLU-124          [-1, 256, 14, 14]               0\n",
            "          Conv2d-125          [-1, 128, 14, 14]          32,768\n",
            "     BatchNorm2d-126          [-1, 128, 14, 14]             256\n",
            "            ReLU-127          [-1, 128, 14, 14]               0\n",
            "          Conv2d-128           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-129          [-1, 288, 14, 14]             576\n",
            "            ReLU-130          [-1, 288, 14, 14]               0\n",
            "          Conv2d-131          [-1, 128, 14, 14]          36,864\n",
            "     BatchNorm2d-132          [-1, 128, 14, 14]             256\n",
            "            ReLU-133          [-1, 128, 14, 14]               0\n",
            "          Conv2d-134           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-135          [-1, 320, 14, 14]             640\n",
            "            ReLU-136          [-1, 320, 14, 14]               0\n",
            "          Conv2d-137          [-1, 128, 14, 14]          40,960\n",
            "     BatchNorm2d-138          [-1, 128, 14, 14]             256\n",
            "            ReLU-139          [-1, 128, 14, 14]               0\n",
            "          Conv2d-140           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-141          [-1, 352, 14, 14]             704\n",
            "            ReLU-142          [-1, 352, 14, 14]               0\n",
            "          Conv2d-143          [-1, 128, 14, 14]          45,056\n",
            "     BatchNorm2d-144          [-1, 128, 14, 14]             256\n",
            "            ReLU-145          [-1, 128, 14, 14]               0\n",
            "          Conv2d-146           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-147          [-1, 384, 14, 14]             768\n",
            "            ReLU-148          [-1, 384, 14, 14]               0\n",
            "          Conv2d-149          [-1, 128, 14, 14]          49,152\n",
            "     BatchNorm2d-150          [-1, 128, 14, 14]             256\n",
            "            ReLU-151          [-1, 128, 14, 14]               0\n",
            "          Conv2d-152           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-153          [-1, 416, 14, 14]             832\n",
            "            ReLU-154          [-1, 416, 14, 14]               0\n",
            "          Conv2d-155          [-1, 128, 14, 14]          53,248\n",
            "     BatchNorm2d-156          [-1, 128, 14, 14]             256\n",
            "            ReLU-157          [-1, 128, 14, 14]               0\n",
            "          Conv2d-158           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-159          [-1, 448, 14, 14]             896\n",
            "            ReLU-160          [-1, 448, 14, 14]               0\n",
            "          Conv2d-161          [-1, 128, 14, 14]          57,344\n",
            "     BatchNorm2d-162          [-1, 128, 14, 14]             256\n",
            "            ReLU-163          [-1, 128, 14, 14]               0\n",
            "          Conv2d-164           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-165          [-1, 480, 14, 14]             960\n",
            "            ReLU-166          [-1, 480, 14, 14]               0\n",
            "          Conv2d-167          [-1, 128, 14, 14]          61,440\n",
            "     BatchNorm2d-168          [-1, 128, 14, 14]             256\n",
            "            ReLU-169          [-1, 128, 14, 14]               0\n",
            "          Conv2d-170           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-171          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-172          [-1, 512, 14, 14]               0\n",
            "          Conv2d-173          [-1, 128, 14, 14]          65,536\n",
            "     BatchNorm2d-174          [-1, 128, 14, 14]             256\n",
            "            ReLU-175          [-1, 128, 14, 14]               0\n",
            "          Conv2d-176           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-177          [-1, 544, 14, 14]           1,088\n",
            "            ReLU-178          [-1, 544, 14, 14]               0\n",
            "          Conv2d-179          [-1, 128, 14, 14]          69,632\n",
            "     BatchNorm2d-180          [-1, 128, 14, 14]             256\n",
            "            ReLU-181          [-1, 128, 14, 14]               0\n",
            "          Conv2d-182           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-183          [-1, 576, 14, 14]           1,152\n",
            "            ReLU-184          [-1, 576, 14, 14]               0\n",
            "          Conv2d-185          [-1, 128, 14, 14]          73,728\n",
            "     BatchNorm2d-186          [-1, 128, 14, 14]             256\n",
            "            ReLU-187          [-1, 128, 14, 14]               0\n",
            "          Conv2d-188           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-189          [-1, 608, 14, 14]           1,216\n",
            "            ReLU-190          [-1, 608, 14, 14]               0\n",
            "          Conv2d-191          [-1, 128, 14, 14]          77,824\n",
            "     BatchNorm2d-192          [-1, 128, 14, 14]             256\n",
            "            ReLU-193          [-1, 128, 14, 14]               0\n",
            "          Conv2d-194           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-195          [-1, 640, 14, 14]           1,280\n",
            "            ReLU-196          [-1, 640, 14, 14]               0\n",
            "          Conv2d-197          [-1, 128, 14, 14]          81,920\n",
            "     BatchNorm2d-198          [-1, 128, 14, 14]             256\n",
            "            ReLU-199          [-1, 128, 14, 14]               0\n",
            "          Conv2d-200           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-201          [-1, 672, 14, 14]           1,344\n",
            "            ReLU-202          [-1, 672, 14, 14]               0\n",
            "          Conv2d-203          [-1, 128, 14, 14]          86,016\n",
            "     BatchNorm2d-204          [-1, 128, 14, 14]             256\n",
            "            ReLU-205          [-1, 128, 14, 14]               0\n",
            "          Conv2d-206           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-207          [-1, 704, 14, 14]           1,408\n",
            "            ReLU-208          [-1, 704, 14, 14]               0\n",
            "          Conv2d-209          [-1, 128, 14, 14]          90,112\n",
            "     BatchNorm2d-210          [-1, 128, 14, 14]             256\n",
            "            ReLU-211          [-1, 128, 14, 14]               0\n",
            "          Conv2d-212           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-213          [-1, 736, 14, 14]           1,472\n",
            "            ReLU-214          [-1, 736, 14, 14]               0\n",
            "          Conv2d-215          [-1, 128, 14, 14]          94,208\n",
            "     BatchNorm2d-216          [-1, 128, 14, 14]             256\n",
            "            ReLU-217          [-1, 128, 14, 14]               0\n",
            "          Conv2d-218           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-219          [-1, 768, 14, 14]           1,536\n",
            "            ReLU-220          [-1, 768, 14, 14]               0\n",
            "          Conv2d-221          [-1, 128, 14, 14]          98,304\n",
            "     BatchNorm2d-222          [-1, 128, 14, 14]             256\n",
            "            ReLU-223          [-1, 128, 14, 14]               0\n",
            "          Conv2d-224           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-225          [-1, 800, 14, 14]           1,600\n",
            "            ReLU-226          [-1, 800, 14, 14]               0\n",
            "          Conv2d-227          [-1, 128, 14, 14]         102,400\n",
            "     BatchNorm2d-228          [-1, 128, 14, 14]             256\n",
            "            ReLU-229          [-1, 128, 14, 14]               0\n",
            "          Conv2d-230           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-231          [-1, 832, 14, 14]           1,664\n",
            "            ReLU-232          [-1, 832, 14, 14]               0\n",
            "          Conv2d-233          [-1, 128, 14, 14]         106,496\n",
            "     BatchNorm2d-234          [-1, 128, 14, 14]             256\n",
            "            ReLU-235          [-1, 128, 14, 14]               0\n",
            "          Conv2d-236           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-237          [-1, 864, 14, 14]           1,728\n",
            "            ReLU-238          [-1, 864, 14, 14]               0\n",
            "          Conv2d-239          [-1, 128, 14, 14]         110,592\n",
            "     BatchNorm2d-240          [-1, 128, 14, 14]             256\n",
            "            ReLU-241          [-1, 128, 14, 14]               0\n",
            "          Conv2d-242           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-243          [-1, 896, 14, 14]           1,792\n",
            "            ReLU-244          [-1, 896, 14, 14]               0\n",
            "          Conv2d-245          [-1, 128, 14, 14]         114,688\n",
            "     BatchNorm2d-246          [-1, 128, 14, 14]             256\n",
            "            ReLU-247          [-1, 128, 14, 14]               0\n",
            "          Conv2d-248           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-249          [-1, 928, 14, 14]           1,856\n",
            "            ReLU-250          [-1, 928, 14, 14]               0\n",
            "          Conv2d-251          [-1, 128, 14, 14]         118,784\n",
            "     BatchNorm2d-252          [-1, 128, 14, 14]             256\n",
            "            ReLU-253          [-1, 128, 14, 14]               0\n",
            "          Conv2d-254           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-255          [-1, 960, 14, 14]           1,920\n",
            "            ReLU-256          [-1, 960, 14, 14]               0\n",
            "          Conv2d-257          [-1, 128, 14, 14]         122,880\n",
            "     BatchNorm2d-258          [-1, 128, 14, 14]             256\n",
            "            ReLU-259          [-1, 128, 14, 14]               0\n",
            "          Conv2d-260           [-1, 32, 14, 14]          36,864\n",
            "     BatchNorm2d-261          [-1, 992, 14, 14]           1,984\n",
            "            ReLU-262          [-1, 992, 14, 14]               0\n",
            "          Conv2d-263          [-1, 128, 14, 14]         126,976\n",
            "     BatchNorm2d-264          [-1, 128, 14, 14]             256\n",
            "            ReLU-265          [-1, 128, 14, 14]               0\n",
            "          Conv2d-266           [-1, 32, 14, 14]          36,864\n",
            "     _DenseBlock-267         [-1, 1024, 14, 14]               0\n",
            "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-269         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-270          [-1, 512, 14, 14]         524,288\n",
            "       AvgPool2d-271            [-1, 512, 7, 7]               0\n",
            "     BatchNorm2d-272            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-273            [-1, 512, 7, 7]               0\n",
            "          Conv2d-274            [-1, 128, 7, 7]          65,536\n",
            "     BatchNorm2d-275            [-1, 128, 7, 7]             256\n",
            "            ReLU-276            [-1, 128, 7, 7]               0\n",
            "          Conv2d-277             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-278            [-1, 544, 7, 7]           1,088\n",
            "            ReLU-279            [-1, 544, 7, 7]               0\n",
            "          Conv2d-280            [-1, 128, 7, 7]          69,632\n",
            "     BatchNorm2d-281            [-1, 128, 7, 7]             256\n",
            "            ReLU-282            [-1, 128, 7, 7]               0\n",
            "          Conv2d-283             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-284            [-1, 576, 7, 7]           1,152\n",
            "            ReLU-285            [-1, 576, 7, 7]               0\n",
            "          Conv2d-286            [-1, 128, 7, 7]          73,728\n",
            "     BatchNorm2d-287            [-1, 128, 7, 7]             256\n",
            "            ReLU-288            [-1, 128, 7, 7]               0\n",
            "          Conv2d-289             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-290            [-1, 608, 7, 7]           1,216\n",
            "            ReLU-291            [-1, 608, 7, 7]               0\n",
            "          Conv2d-292            [-1, 128, 7, 7]          77,824\n",
            "     BatchNorm2d-293            [-1, 128, 7, 7]             256\n",
            "            ReLU-294            [-1, 128, 7, 7]               0\n",
            "          Conv2d-295             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-296            [-1, 640, 7, 7]           1,280\n",
            "            ReLU-297            [-1, 640, 7, 7]               0\n",
            "          Conv2d-298            [-1, 128, 7, 7]          81,920\n",
            "     BatchNorm2d-299            [-1, 128, 7, 7]             256\n",
            "            ReLU-300            [-1, 128, 7, 7]               0\n",
            "          Conv2d-301             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-302            [-1, 672, 7, 7]           1,344\n",
            "            ReLU-303            [-1, 672, 7, 7]               0\n",
            "          Conv2d-304            [-1, 128, 7, 7]          86,016\n",
            "     BatchNorm2d-305            [-1, 128, 7, 7]             256\n",
            "            ReLU-306            [-1, 128, 7, 7]               0\n",
            "          Conv2d-307             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-308            [-1, 704, 7, 7]           1,408\n",
            "            ReLU-309            [-1, 704, 7, 7]               0\n",
            "          Conv2d-310            [-1, 128, 7, 7]          90,112\n",
            "     BatchNorm2d-311            [-1, 128, 7, 7]             256\n",
            "            ReLU-312            [-1, 128, 7, 7]               0\n",
            "          Conv2d-313             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-314            [-1, 736, 7, 7]           1,472\n",
            "            ReLU-315            [-1, 736, 7, 7]               0\n",
            "          Conv2d-316            [-1, 128, 7, 7]          94,208\n",
            "     BatchNorm2d-317            [-1, 128, 7, 7]             256\n",
            "            ReLU-318            [-1, 128, 7, 7]               0\n",
            "          Conv2d-319             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-320            [-1, 768, 7, 7]           1,536\n",
            "            ReLU-321            [-1, 768, 7, 7]               0\n",
            "          Conv2d-322            [-1, 128, 7, 7]          98,304\n",
            "     BatchNorm2d-323            [-1, 128, 7, 7]             256\n",
            "            ReLU-324            [-1, 128, 7, 7]               0\n",
            "          Conv2d-325             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-326            [-1, 800, 7, 7]           1,600\n",
            "            ReLU-327            [-1, 800, 7, 7]               0\n",
            "          Conv2d-328            [-1, 128, 7, 7]         102,400\n",
            "     BatchNorm2d-329            [-1, 128, 7, 7]             256\n",
            "            ReLU-330            [-1, 128, 7, 7]               0\n",
            "          Conv2d-331             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-332            [-1, 832, 7, 7]           1,664\n",
            "            ReLU-333            [-1, 832, 7, 7]               0\n",
            "          Conv2d-334            [-1, 128, 7, 7]         106,496\n",
            "     BatchNorm2d-335            [-1, 128, 7, 7]             256\n",
            "            ReLU-336            [-1, 128, 7, 7]               0\n",
            "          Conv2d-337             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-338            [-1, 864, 7, 7]           1,728\n",
            "            ReLU-339            [-1, 864, 7, 7]               0\n",
            "          Conv2d-340            [-1, 128, 7, 7]         110,592\n",
            "     BatchNorm2d-341            [-1, 128, 7, 7]             256\n",
            "            ReLU-342            [-1, 128, 7, 7]               0\n",
            "          Conv2d-343             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-344            [-1, 896, 7, 7]           1,792\n",
            "            ReLU-345            [-1, 896, 7, 7]               0\n",
            "          Conv2d-346            [-1, 128, 7, 7]         114,688\n",
            "     BatchNorm2d-347            [-1, 128, 7, 7]             256\n",
            "            ReLU-348            [-1, 128, 7, 7]               0\n",
            "          Conv2d-349             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-350            [-1, 928, 7, 7]           1,856\n",
            "            ReLU-351            [-1, 928, 7, 7]               0\n",
            "          Conv2d-352            [-1, 128, 7, 7]         118,784\n",
            "     BatchNorm2d-353            [-1, 128, 7, 7]             256\n",
            "            ReLU-354            [-1, 128, 7, 7]               0\n",
            "          Conv2d-355             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-356            [-1, 960, 7, 7]           1,920\n",
            "            ReLU-357            [-1, 960, 7, 7]               0\n",
            "          Conv2d-358            [-1, 128, 7, 7]         122,880\n",
            "     BatchNorm2d-359            [-1, 128, 7, 7]             256\n",
            "            ReLU-360            [-1, 128, 7, 7]               0\n",
            "          Conv2d-361             [-1, 32, 7, 7]          36,864\n",
            "     BatchNorm2d-362            [-1, 992, 7, 7]           1,984\n",
            "            ReLU-363            [-1, 992, 7, 7]               0\n",
            "          Conv2d-364            [-1, 128, 7, 7]         126,976\n",
            "     BatchNorm2d-365            [-1, 128, 7, 7]             256\n",
            "            ReLU-366            [-1, 128, 7, 7]               0\n",
            "          Conv2d-367             [-1, 32, 7, 7]          36,864\n",
            "     _DenseBlock-368           [-1, 1024, 7, 7]               0\n",
            "     BatchNorm2d-369           [-1, 1024, 7, 7]           2,048\n",
            "        Identity-370                 [-1, 1024]               0\n",
            "        DenseNet-371                 [-1, 1024]               0\n",
            "          Linear-372                  [-1, 120]         123,000\n",
            "================================================================\n",
            "Total params: 7,076,856\n",
            "Trainable params: 7,076,856\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 305.31\n",
            "Params size (MB): 27.00\n",
            "Estimated Total Size (MB): 332.88\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Pe7TT5XpdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda'\n",
        "cls_criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da_nGeoC6GN_",
        "colab_type": "code",
        "outputId": "f4cbba68-d915-4e61-b4b8-cdc776cef97a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "iteration = 0\n",
        "acc_all = list()\n",
        "loss_all = list()\n",
        "dataloaders = {'train':train_loader} \n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print('')\n",
        "    print(f\"--- Epoch {epoch} ---\")\n",
        "    phase1 = dataloaders.keys()\n",
        "    for phase in phase1:\n",
        "        print('')\n",
        "        print(f\"--- Phase {phase} ---\")\n",
        "        epoch_metrics = {\"loss\": [], \"acc\": []}\n",
        "        for batch_i, (X, y) in enumerate(dataloaders[phase]):\n",
        "            image_sequences = Variable(X.to(device), requires_grad=True)\n",
        "            labels = Variable(y.to(device), requires_grad=False)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(image_sequences)\n",
        "            loss = cls_criterion(predictions, labels)\n",
        "            acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_metrics[\"loss\"].append(loss.item())\n",
        "            epoch_metrics[\"acc\"].append(acc)\n",
        "            batches_done = epoch * len(dataloaders[phase]) + batch_i\n",
        "            batches_left = num_epochs * len(dataloaders[phase]) - batches_done\n",
        "            sys.stdout.write(\n",
        "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n",
        "                    % (\n",
        "                        epoch,\n",
        "                        num_epochs,\n",
        "                        batch_i,\n",
        "                        len(dataloaders[phase]),\n",
        "                        loss.item(),\n",
        "                        np.mean(epoch_metrics[\"loss\"]),\n",
        "                        acc,\n",
        "                        np.mean(epoch_metrics[\"acc\"]),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # Empty cache\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "            \n",
        "        print('')\n",
        "        print('{} , acc: {}'.format(phase,np.mean(epoch_metrics[\"acc\"])))\n",
        "        torch.save(model.state_dict(),'/content/model.h5'.format(epoch))\n",
        "        if(phase=='train'):\n",
        "          acc_all.append(np.mean(epoch_metrics[\"acc\"]))\n",
        "          loss_all.append(np.mean(epoch_metrics[\"loss\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 0 ---\n",
            "\n",
            "--- Phase train ---\n",
            "[Epoch 0/10] [Batch 638/639] [Loss: 4.806834 (5.048705), Acc: 0.00% (0.91%)]\n",
            "train , acc: 0.9096244131455399\n",
            "\n",
            "--- Epoch 1 ---\n",
            "\n",
            "--- Phase train ---\n",
            "[Epoch 1/10] [Batch 638/639] [Loss: 4.850374 (4.753492), Acc: 0.00% (1.59%)]\n",
            "train , acc: 1.5942879499217528\n",
            "\n",
            "--- Epoch 2 ---\n",
            "\n",
            "--- Phase train ---\n",
            "[Epoch 2/10] [Batch 638/639] [Loss: 4.590879 (4.646283), Acc: 0.00% (2.12%)]\n",
            "train , acc: 2.12245696400626\n",
            "\n",
            "--- Epoch 3 ---\n",
            "\n",
            "--- Phase train ---\n",
            "[Epoch 3/10] [Batch 638/639] [Loss: 4.700273 (4.468488), Acc: 7.14% (3.39%)]\n",
            "train , acc: 3.3855913257321704\n",
            "\n",
            "--- Epoch 4 ---\n",
            "\n",
            "--- Phase train ---\n",
            "[Epoch 4/10] [Batch 330/639] [Loss: 4.224683 (4.359656), Acc: 6.25% (4.72%)]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SikjVFxMJftZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = True\n",
        "train_loader = DataLoader(train_data,batch_size = 16,num_workers = 4,shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ13H8CDJevD",
        "colab_type": "code",
        "outputId": "32e084c2-0b14-41dc-bef8-010b202e8752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "iteration = 0\n",
        "acc_all = list()\n",
        "loss_all = list()\n",
        "dataloaders = {'train':train_loader} \n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print('')\n",
        "    print(f\"--- Epoch {epoch} ---\")\n",
        "    phase1 = dataloaders.keys()\n",
        "    for phase in phase1:\n",
        "        print('')\n",
        "        print(f\"--- Phase {phase} ---\")\n",
        "        epoch_metrics = {\"loss\": [], \"acc\": []}\n",
        "        for batch_i, (X, y) in enumerate(dataloaders[phase]):\n",
        "            image_sequences = Variable(X.to(device), requires_grad=True)\n",
        "            labels = Variable(y.to(device), requires_grad=False)\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(image_sequences)\n",
        "            loss = cls_criterion(predictions, labels)\n",
        "            acc = 100 * (predictions.detach().argmax(1) == labels).cpu().numpy().mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_metrics[\"loss\"].append(loss.item())\n",
        "            epoch_metrics[\"acc\"].append(acc)\n",
        "            batches_done = epoch * len(dataloaders[phase]) + batch_i\n",
        "            batches_left = num_epochs * len(dataloaders[phase]) - batches_done\n",
        "            sys.stdout.write(\n",
        "                    \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n",
        "                    % (\n",
        "                        epoch,\n",
        "                        num_epochs,\n",
        "                        batch_i,\n",
        "                        len(dataloaders[phase]),\n",
        "                        loss.item(),\n",
        "                        np.mean(epoch_metrics[\"loss\"]),\n",
        "                        acc,\n",
        "                        np.mean(epoch_metrics[\"acc\"]),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "                # Empty cache\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            \n",
        "        print('')\n",
        "        print('{} , acc: {}'.format(phase,np.mean(epoch_metrics[\"acc\"])))\n",
        "        torch.save(model.state_dict(),'/content/model.h5'.format(epoch))\n",
        "        if(phase=='train'):\n",
        "          acc_all.append(np.mean(epoch_metrics[\"acc\"]))\n",
        "          loss_all.append(np.mean(epoch_metrics[\"loss\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--- Epoch 0 ---\n",
            "\n",
            "--- Phase train ---\n",
            "[Epoch 0/10] [Batch 638/639] [Loss: 4.847332 (4.993830), Acc: 0.00% (1.04%)]\n",
            "train , acc: 1.0367762128325508\n",
            "\n",
            "--- Epoch 1 ---\n",
            "\n",
            "--- Phase train ---\n",
            "[Epoch 1/10] [Batch 255/639] [Loss: 4.842217 (4.836545), Acc: 0.00% (0.85%)]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-6f7b6b4da108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mepoch_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}